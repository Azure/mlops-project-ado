{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "mlflow.end_run()\r\n",
        "\r\n",
        "run.complete()"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace, Experiment, Run, Model\r\n",
        "\r\n",
        "import mlflow\r\n",
        "import mlflow.pyfunc\r\n",
        "from mlflow.tracking import MlflowClient\r\n",
        "\r\n",
        "ws = Workspace.from_config()\r\n",
        "experiment = Experiment(workspace=ws, name=\"taxi-evaluate\")\r\n",
        "run = experiment.start_logging()\r\n",
        "\r\n",
        "client = MlflowClient()\r\n",
        "\r\n",
        "mlflow.start_run()\r\n",
        "mlflow.active_run()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": "<ActiveRun: >"
          },
          "metadata": {}
        }
      ],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.active_run().info\r\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": "<RunInfo: artifact_uri='azureml://experiments/Default/runs/23d486cc-0a2c-43c8-ae76-e7f6860a1fe8/artifacts', end_time=None, experiment_id='c4ceba0c-fe4f-4dd1-b24e-8f9bb61a8514', lifecycle_stage='active', run_id='23d486cc-0a2c-43c8-ae76-e7f6860a1fe8', run_uuid='23d486cc-0a2c-43c8-ae76-e7f6860a1fe8', start_time=1654857384891, status='RUNNING', user_id='78dcac7d-8b2f-43e4-a89d-01ca68254582'>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.get_run(mlflow.active_run().info.run_id)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 22,
          "data": {
            "text/plain": "<Run: data=<RunData: metrics={}, params={}, tags={'mlflow.rootRunId': '23d486cc-0a2c-43c8-ae76-e7f6860a1fe8',\n 'mlflow.runName': 'frosty_fly_6nf864bb',\n 'mlflow.source.name': '/anaconda/envs/azureml_py38/lib/python3.8/site-packages/ipykernel_launcher.py',\n 'mlflow.source.type': 'LOCAL',\n 'mlflow.user': 'Maggie Mhanna'}>, info=<RunInfo: artifact_uri='azureml://experiments/Default/runs/23d486cc-0a2c-43c8-ae76-e7f6860a1fe8/artifacts', end_time=None, experiment_id='c4ceba0c-fe4f-4dd1-b24e-8f9bb61a8514', lifecycle_stage='active', run_id='23d486cc-0a2c-43c8-ae76-e7f6860a1fe8', run_uuid='23d486cc-0a2c-43c8-ae76-e7f6860a1fe8', start_time=1654857384891, status='RUNNING', user_id='78dcac7d-8b2f-43e4-a89d-01ca68254582'>>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {
            "text/plain": "Run(Experiment: taxi-evaluate,\nId: 164b478f-8260-41ea-9fdb-46c3e9fb79e2,\nType: None,\nStatus: Running)",
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>taxi-evaluate</td><td>164b478f-8260-41ea-9fdb-46c3e9fb79e2</td><td></td><td>Running</td><td><a href=\"https://ml.azure.com/runs/164b478f-8260-41ea-9fdb-46c3e9fb79e2?wsid=/subscriptions/2f091423-f84d-4062-8e67-1437a0c50045/resourcegroups/rg-mlops-template-dev/workspaces/mlopstmpldev&amp;tid=72f988bf-86f1-41af-91ab-2d7cd011db47\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run.Run?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\r\n",
        "from pathlib import Path\r\n",
        "import os\r\n",
        "import pickle\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from matplotlib import pyplot as plt\r\n",
        "\r\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\r\n",
        "\r\n",
        "from azureml.core import Workspace, Experiment, Run, Model\r\n",
        "\r\n",
        "from fairlearn.metrics._group_metric_set import _create_group_metric_set\r\n",
        "from azureml.contrib.fairness import upload_dashboard_dictionary, download_dashboard_by_upload_id\r\n",
        "\r\n",
        "from interpret_community import TabularExplainer\r\n",
        "from azureml.interpret import ExplanationClient\r\n",
        "\r\n",
        "import mlflow"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"taxi-model\"\r\n",
        "model_input = \"/tmp/train\"\r\n",
        "prepared_data = \"/tmp/prep\"\r\n",
        "evaluation_output = \"/tmp/evaluate\"\r\n",
        "\r\n",
        "os.makedirs(evaluation_output, exist_ok = True)\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET_COL = \"cost\"\r\n",
        "\r\n",
        "NUMERIC_COLS = [\r\n",
        "    \"distance\",\r\n",
        "    \"dropoff_latitude\",\r\n",
        "    \"dropoff_longitude\",\r\n",
        "    \"passengers\",\r\n",
        "    \"pickup_latitude\",\r\n",
        "    \"pickup_longitude\",\r\n",
        "    \"pickup_weekday\",\r\n",
        "    \"pickup_month\",\r\n",
        "    \"pickup_monthday\",\r\n",
        "    \"pickup_hour\",\r\n",
        "    \"pickup_minute\",\r\n",
        "    \"pickup_second\",\r\n",
        "    \"dropoff_weekday\",\r\n",
        "    \"dropoff_month\",\r\n",
        "    \"dropoff_monthday\",\r\n",
        "    \"dropoff_hour\",\r\n",
        "    \"dropoff_minute\",\r\n",
        "    \"dropoff_second\",\r\n",
        "]\r\n",
        "\r\n",
        "CAT_NOM_COLS = [\r\n",
        "    \"store_forward\",\r\n",
        "    \"vendor\",\r\n",
        "]\r\n",
        "\r\n",
        "CAT_ORD_COLS = [\r\n",
        "]\r\n",
        "\r\n",
        "SENSITIVE_COLS = [\"vendor\"] # for fairlearn dashborad\r\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ws = Workspace.from_config()\r\n",
        "experiment = Experiment(workspace=ws, name=\"taxi-evaluate\")\r\n",
        "run = experiment.start_logging()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    lines = [\r\n",
        "        f\"Model path: {model_input}\",\r\n",
        "        f\"Test data path: {prepared_data}\",\r\n",
        "        f\"evaluation output path: {evaluation_output}\",\r\n",
        "    ]\r\n",
        "\r\n",
        "    for line in lines:\r\n",
        "        print(line)\r\n",
        "\r\n",
        "    # ---------------- Model Evaluation ---------------- #\r\n",
        "\r\n",
        "    # Load the test data\r\n",
        "\r\n",
        "    print(\"mounted_path files: \")\r\n",
        "    arr = os.listdir(prepared_data)\r\n",
        "\r\n",
        "    train_data = pd.read_csv((Path(prepared_data) / \"train.csv\"))\r\n",
        "    test_data = pd.read_csv((Path(prepared_data) / \"test.csv\"))\r\n",
        "\r\n",
        "    y_train = train_data[TARGET_COL]\r\n",
        "    X_train = train_data[NUMERIC_COLS + CAT_NOM_COLS + CAT_ORD_COLS]\r\n",
        "\r\n",
        "    y_test = test_data[TARGET_COL]\r\n",
        "    X_test = test_data[NUMERIC_COLS + CAT_NOM_COLS + CAT_ORD_COLS]\r\n",
        "\r\n",
        "    # Load the model from input port\r\n",
        "    model = pickle.load(open((Path(model_input) / \"model.pkl\"), \"rb\"))\r\n",
        "\r\n",
        "    # Get predictions to y_test (y_test)\r\n",
        "    yhat_test = model.predict(X_test)\r\n",
        "\r\n",
        "    # Save the output data with feature columns, predicted cost, and actual cost in csv file\r\n",
        "    output_data = X_test.copy()\r\n",
        "    output_data[\"real_label\"] = y_test\r\n",
        "    output_data[\"predicted_label\"] = yhat_test\r\n",
        "    output_data.to_csv((Path(evaluation_output) / \"predictions.csv\"))\r\n",
        "\r\n",
        "    # Evaluate Model performance with the test set\r\n",
        "    r2 = r2_score(y_test, yhat_test)\r\n",
        "    mse = mean_squared_error(y_test, yhat_test)\r\n",
        "    rmse = np.sqrt(mse)\r\n",
        "    mae = mean_absolute_error(y_test, yhat_test)\r\n",
        "\r\n",
        "    # Print score report to a text file\r\n",
        "    (Path(evaluation_output) / \"score.txt\").write_text(\r\n",
        "        \"Scored with the following model:\\n{}\".format(model)\r\n",
        "    )\r\n",
        "    with open((Path(evaluation_output) / \"score.txt\"), \"a\") as f:\r\n",
        "        f.write(\"Mean squared error: %.2f \\n\" % mse)\r\n",
        "        f.write(\"Root mean squared error: %.2f \\n\" % rmse)\r\n",
        "        f.write(\"Mean absolute error: %.2f \\n\" % mae)\r\n",
        "        f.write(\"Coefficient of determination: %.2f \\n\" % r2)\r\n",
        "\r\n",
        "\r\n",
        "    mlflow.log_metric(\"test r2\", r2)\r\n",
        "    mlflow.log_metric(\"test mse\", mse)\r\n",
        "    mlflow.log_metric(\"test rmse\", rmse)\r\n",
        "    mlflow.log_metric(\"test mae\", mae)\r\n",
        "\r\n",
        "    # Visualize results\r\n",
        "    plt.scatter(y_test, yhat_test,  color='black')\r\n",
        "    plt.plot(y_test, y_test, color='blue', linewidth=3)\r\n",
        "    plt.xlabel(\"Real value\")\r\n",
        "    plt.ylabel(\"Predicted value\")\r\n",
        "    plt.title(\"Comparing Model Predictions to Real values - Test Data\")\r\n",
        "    plt.savefig(\"/tmp/evaluate/predictions.png\")\r\n",
        "    mlflow.log_artifact(\"/tmp/evaluate/predictions.png\")\r\n",
        "\r\n",
        "    # -------------------- Promotion ------------------- #\r\n",
        "    scores = {}\r\n",
        "    predictions = {}\r\n",
        "    score = r2_score(y_test, yhat_test) # current model\r\n",
        "    for model_run in Model.list(ws):\r\n",
        "        if model_run.name == model_name:\r\n",
        "            model_path = Model.download(model_run, exist_ok=True)\r\n",
        "            mdl = pickle.load(open((Path(model_path) / \"model.pkl\"), \"rb\"))\r\n",
        "            predictions[model_run.id] = mdl.predict(X_test)\r\n",
        "            scores[model_run.id] = r2_score(y_test, predictions[model_run.id])\r\n",
        "        \r\n",
        "    print(scores)\r\n",
        "    if scores:\r\n",
        "        if score >= max(list(scores.values())):\r\n",
        "            deploy_flag = 1\r\n",
        "        else:\r\n",
        "            deploy_flag = 0\r\n",
        "    else:\r\n",
        "        deploy_flag = 1\r\n",
        "    print(\"Deploy flag: \",deploy_flag)\r\n",
        "\r\n",
        "    with open((Path(evaluation_output) / \"deploy_flag\"), 'w') as f:\r\n",
        "        f.write('%d' % int(deploy_flag))\r\n",
        "                \r\n",
        "    scores[\"current model\"] = score\r\n",
        "    perf_comparison_plot = pd.DataFrame(scores, index=[\"r2 score\"]).plot(kind='bar', figsize=(15, 10))\r\n",
        "    perf_comparison_plot.figure.savefig(\"/tmp/evaluate/perf_comparison.png\")\r\n",
        "    perf_comparison_plot.figure.savefig(Path(evaluation_output) / \"perf_comparison.png\")\r\n",
        "    \r\n",
        "    mlflow.log_metric(\"deploy flag\", int(deploy_flag))\r\n",
        "    mlflow.log_artifact(\"/tmp/evaluate/perf_comparison.png\")\r\n",
        "    \r\n",
        "\r\n",
        "    # -------------------- FAIRNESS ------------------- #\r\n",
        "    # Calculate Fairness Metrics over Sensitive Features\r\n",
        "    # Create a dictionary of model(s) you want to assess for fairness \r\n",
        "    \r\n",
        "    sf = { col: X_test[[col]] for col in SENSITIVE_COLS }\r\n",
        "    predictions[\"currrent model\"] = [x for x in model.predict(X_test)]\r\n",
        "    \r\n",
        "    dash_dict_all = _create_group_metric_set(y_true=y_test,\r\n",
        "                                             predictions=predictions,\r\n",
        "                                             sensitive_features=sf,\r\n",
        "                                             prediction_type='regression',\r\n",
        "                                            )\r\n",
        "    \r\n",
        "    # Upload the dashboard to Azure Machine Learning\r\n",
        "    dashboard_title = \"Fairness insights Comparison of Models\"\r\n",
        "\r\n",
        "    # Set validate_model_ids parameter of upload_dashboard_dictionary to False \r\n",
        "    # if you have not registered your model(s)\r\n",
        "    upload_id = upload_dashboard_dictionary(run,\r\n",
        "                                            dash_dict_all,\r\n",
        "                                            dashboard_name=dashboard_title,\r\n",
        "                                            validate_model_ids=False)\r\n",
        "    print(\"\\nUploaded to id: {0}\\n\".format(upload_id))\r\n",
        "\r\n",
        "    \r\n",
        "    # -------------------- Explainability ------------------- #\r\n",
        "    tabular_explainer = TabularExplainer(model,\r\n",
        "                                   initialization_examples=X_train,\r\n",
        "                                   features=X_train.columns)\r\n",
        "    \r\n",
        "    # save explainer                                 \r\n",
        "    #joblib.dump(tabular_explainer, os.path.join(tabular_explainer, \"explainer\"))\r\n",
        "\r\n",
        "    # find global explanations for feature importance\r\n",
        "    # you can use the training data or the test data here, \r\n",
        "    # but test data would allow you to use Explanation Exploration\r\n",
        "    global_explanation = tabular_explainer.explain_global(X_test)\r\n",
        "\r\n",
        "    # sorted feature importance values and feature names\r\n",
        "    sorted_global_importance_values = global_explanation.get_ranked_global_values()\r\n",
        "    sorted_global_importance_names = global_explanation.get_ranked_global_names()\r\n",
        "\r\n",
        "    print(\"Explainability feature importance:\")\r\n",
        "    # alternatively, you can print out a dictionary that holds the top K feature names and values\r\n",
        "    global_explanation.get_feature_importance_dict()\r\n",
        "    \r\n",
        "    client = ExplanationClient.from_run(run)\r\n",
        "    client.upload_model_explanation(global_explanation, comment='global explanation: all features')\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == \"__main__\":\r\n",
        "    main()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run.complete()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /tmp/evaluate"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python38-azureml"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}